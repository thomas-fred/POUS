{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f791f9-248f-40e3-858a-331d7cd07ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1865022/794942655.py:3: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simpson\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1995bea8-24f6-4e66-9d96-136c285f691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')  # for cool points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a188b2-89e5-4877-b074-2df159e8cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "\n",
    "outage_threshold = 0.05  # OutageFraction above this is considered an outage\n",
    "resample_freq = \"12H\"  # resample raw hourly data to this resolution, then check for outage state\n",
    "start_buffer = \"2D\"  # when plotting outage timeseries, start this delta ahead of first outage period\n",
    "end_buffer = \"1W\"  # when plotting outage timeseries, end this delta after last outage period\n",
    "\n",
    "# outage clustering (DBSCAN) parameters\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "# temporal clustering\n",
    "temp_epsilon_days = 0.7\n",
    "temp_min_samples = 2\n",
    "# spatial clustering\n",
    "geo_epsilon_deg = 0.6\n",
    "geo_min_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e55c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths (except for plots, defined later)\n",
    "\n",
    "years = list(range(2017, 2023))  # years we have POUS data for\n",
    "\n",
    "root_dir = \"data\"\n",
    "states = pd.read_csv(\n",
    "    os.path.join(\n",
    "        root_dir,\n",
    "        \"raw\",\n",
    "        \"states\",\n",
    "        \"state_codes.csv\"\n",
    "    )\n",
    ").set_index(\"state_fips_code\")\n",
    "county_boundaries: gpd.GeoDataFrame = gpd.read_file(\n",
    "    os.path.join(\n",
    "        root_dir,\n",
    "        \"raw\",\n",
    "        \"counties\",\n",
    "        \"cb_2018_us_county_500k.shp\"\n",
    "    )\n",
    ")\n",
    "all_counties_hourly_path = os.path.join(\n",
    "    root_dir,\n",
    "    \"processed\",\n",
    "    \"outage\",\n",
    "    f\"all_counties_hourly.parquet\"\n",
    ")\n",
    "outage_integrals_path = os.path.join(\n",
    "    root_dir,\n",
    "    \"processed\",\n",
    "    \"outage\",\n",
    "    f\"{resample_freq}_county_outage_integrals.csv\"\n",
    ")\n",
    "outage_attr_path = os.path.join(\n",
    "    root_dir,\n",
    "    \"processed\",\n",
    "    \"outage\",\n",
    "    f\"{resample_freq}_{outage_threshold}_outage_attributes_for_clustering.csv\"\n",
    ")\n",
    "plot_dir = os.path.join(\n",
    "    \"plots\",\n",
    "    \"outage_timeseries_county_grouped_dbscan\",\n",
    "    f\"resample_{resample_freq}\",\n",
    "    f\"threshold_{outage_threshold}\",\n",
    "    f\"{temp_epsilon_days=:.1f}\",\n",
    "    f\"{temp_min_samples=:d}\",\n",
    "    f\"{geo_epsilon_deg=:.1f}\",\n",
    "    f\"{geo_min_samples=:d}\",\n",
    ")\n",
    "duration_dir = os.path.join(\n",
    "    root_dir,\n",
    "    \"processed\",\n",
    "    \"outage\",\n",
    "    \"durations\",\n",
    "    f\"resample_{resample_freq}\",\n",
    "    f\"threshold_{outage_threshold}\",\n",
    "    f\"{temp_epsilon_days=:.1f}\",\n",
    "    f\"{temp_min_samples=:d}\",\n",
    "    f\"{geo_epsilon_deg=:.1f}\",\n",
    "    f\"{geo_min_samples=:d}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88657239-b71e-4ccd-99a7-6efded7367d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hourly data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(all_counties_hourly_path):\n",
    "    print(\"Loading hourly data\")\n",
    "    all_counties_hourly = pd.read_parquet(all_counties_hourly_path)\n",
    "else:\n",
    "    print(\"Building hourly data\")\n",
    "    # read source outage data\n",
    "    data_by_year = {}\n",
    "    for year in years:\n",
    "        processed_path = os.path.join(root_dir, f\"processed/outage/{year}.parquet\")\n",
    "        data = pd.read_parquet(processed_path)\n",
    "        data.OutageFraction = np.clip(data.OutageFraction, 0, 1)\n",
    "        data_by_year[year] = data\n",
    "        \n",
    "    # another view of source data, concat into single dataframe\n",
    "    all_counties_hourly = pd.concat(data_by_year).drop(columns=[\"CustomersTracked\", \"CustomersOut\"])\n",
    "    all_counties_hourly = all_counties_hourly.droplevel(0)\n",
    "    all_counties_hourly.to_parquet(all_counties_hourly_path)\n",
    "    \n",
    "# find set of all counties in data\n",
    "counties = sorted(set(all_counties_hourly.index.get_level_values(\"CountyFIPS\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7382718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct complete timeseries of outage data\n",
    "# resample to resample_freq and take mean of OutageFraction\n",
    "# save to disk as cache\n",
    "\n",
    "if os.path.exists(outage_integrals_path):\n",
    "    print(\"Loading resampled data\")\n",
    "    df = pd.read_csv(outage_integrals_path, dtype={\"CountyFIPS\": str}) \n",
    "    \n",
    "else:\n",
    "    print(\"Building resampled data\")\n",
    "    resampled_data_by_year = []\n",
    "    for county_code in tqdm(counties):\n",
    "\n",
    "        try:\n",
    "            data = all_counties_hourly.loc(axis=0)[:, county_code].reset_index(level=\"CountyFIPS\")\n",
    "            complete_index = pd.date_range(f\"{min(years)}-04-01\", f\"{max(years)}-10-31\", freq=\"1H\")\n",
    "            data = data.reindex(index=complete_index, fill_value=0)\n",
    "            data.index.name = \"RecordDateTime\"\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        data = data.drop(columns=[\"CountyFIPS\"]).resample(resample_freq).mean()\n",
    "        data[\"CountyFIPS\"] = county_code\n",
    "        resampled_data_by_year.append(data)\n",
    "            \n",
    "    df = pd.concat(resampled_data_by_year)\n",
    "    df.to_csv(outage_integrals_path)\n",
    "    \n",
    "df.RecordDateTime = pd.to_datetime(df.RecordDateTime)\n",
    "df = df.set_index(\"RecordDateTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77345e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a table of outages\n",
    "# look through resampled data and identify periods of extended poor service for each county\n",
    "\n",
    "if os.path.exists(outage_attr_path):\n",
    "    print(\"Loading outage attributes\")\n",
    "    outage_attributes = pd.read_csv(outage_attr_path, dtype={\"CountyFIPS\": str})\n",
    "\n",
    "else:\n",
    "    print(\"Building outage attributes\")\n",
    "    data_start: pd.Timestamp = pd.to_datetime(f\"{min(years)}-04-01\")\n",
    "    \n",
    "    # take the resampled data and filter to periods with OutageFraction above a threshold\n",
    "    outages = df[df.OutageFraction > outage_threshold]\n",
    "\n",
    "    # duration of single resampling period in nanoseconds\n",
    "    length_of_resample_period_ns = pd.Timedelta(resample_freq).total_seconds() * 1E9\n",
    "\n",
    "    outage_attributes = []\n",
    "    for county_code in tqdm(set(outages.CountyFIPS)):\n",
    "\n",
    "        county_outages_resampled: pd.DataFrame = outages[outages.CountyFIPS == county_code]\n",
    "        county_data_hourly: pd.DataFrame = all_counties_hourly.loc[(slice(None), county_code), :]\n",
    "\n",
    "        # picking out runs of resampled outage periods\n",
    "        start = 0\n",
    "        outage_period_resampled_indicies: list[tuple[int, int]] = []\n",
    "        for i, time_gap_ns in enumerate(np.diff(county_outages_resampled.index.values)):\n",
    "            if np.abs((float(time_gap_ns) / length_of_resample_period_ns) - 1) < 0.01:\n",
    "                outage_period_resampled_indicies.append((start, i))\n",
    "                start = i + 1\n",
    "\n",
    "        county_centroid = county_boundaries.set_index(\"GEOID\").loc[county_code].geometry.centroid\n",
    "        if outage_period_resampled_indicies:\n",
    "            print(outage_period_resampled_indicies)\n",
    "        \n",
    "        for period_indicies in outage_period_resampled_indicies:\n",
    "\n",
    "            start_i, end_i = period_indicies\n",
    "            n_periods: int = end_i - start_i\n",
    "\n",
    "            # check outage is at least 1 resample period long\n",
    "            if n_periods >= 1:\n",
    "\n",
    "                # retrieve indicies of resampled run of outage periods\n",
    "                group_datetimeindex: pd.DatetimeIndex = county_outages_resampled.iloc[start_i: end_i + 1].index\n",
    "\n",
    "                outage_attributes.append(\n",
    "                    (\n",
    "                        county_code,\n",
    "                        group_datetimeindex[0],\n",
    "                        (group_datetimeindex[0] - data_start).value / (1E9 * 60 * 60 * 24),\n",
    "                        group_datetimeindex[0].year,\n",
    "                        group_datetimeindex[0].day_of_year,\n",
    "                        n_periods,\n",
    "                        county_centroid.x,\n",
    "                        county_centroid.y\n",
    "                    )\n",
    "                )\n",
    "                  \n",
    "    outage_attributes = pd.DataFrame(\n",
    "        outage_attributes,\n",
    "        columns=[\n",
    "            \"CountyFIPS\",\n",
    "            \"start\",\n",
    "            \"days_since_data_start\",\n",
    "            \"year\",\n",
    "            \"day_of_year\",\n",
    "            \"n_periods\",\n",
    "            \"county_long\",\n",
    "            \"county_lat\"\n",
    "        ]\n",
    "    )\n",
    "    outage_attributes.to_csv(outage_attr_path, index=False)\n",
    "    \n",
    "outage_attributes = outage_attributes.sort_values(\"days_since_data_start\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_admin_name(\n",
    "    county_code: str,\n",
    "    county_boundaries: gpd.GeoDataFrame,\n",
    "    states: pd.DataFrame\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Lookup county name, containing state name and code, given a 5-digit string county code.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        county_admin_data: pd.Series = \\\n",
    "            county_boundaries.sort_values(\"GEOID\").set_index(\"GEOID\").loc[county_code, :]\n",
    "        state_code: str = county_admin_data.STATEFP\n",
    "        state_name: str = states.loc[int(state_code), \"state_name\"]\n",
    "        county_name: str = county_admin_data.NAME\n",
    "    except Exception as e:\n",
    "        state_name = \"-\"\n",
    "        county_name = \"-\"\n",
    "        \n",
    "    return county_name, state_name, state_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21850b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pairwise distance in time between outage events\n",
    "\n",
    "arr = outage_attributes.days_since_data_start.values\n",
    "distance = np.abs(arr - arr[:, None])\n",
    "f, ax = plt.subplots()\n",
    "f.subplots_adjust(right=0.9)\n",
    "cbar_ax = f.add_axes([0.85, 0.11, 0.04, 0.77])\n",
    "img = ax.imshow(distance)\n",
    "f.colorbar(img, cax=cbar_ax, label=\"Time [days]\")\n",
    "ax.set_title(\"Pairwise distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b208e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster events temporally using the pairwise distance matrix\n",
    "\n",
    "def temporal_cluster(distance: np.ndarray, epsilon_days: int, min_samples: int):\n",
    "    dbscan = DBSCAN(\n",
    "        eps=epsilon_days,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"precomputed\",\n",
    "    )\n",
    "    dbscan.fit(distance)\n",
    "    return pd.Series(dbscan.labels_)\n",
    "\n",
    "outage_attributes[\"time_cluster_id\"] = temporal_cluster(distance, temp_epsilon_days, temp_min_samples)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,3))\n",
    "for cluster_id in set(outage_attributes.time_cluster_id):\n",
    "    data = outage_attributes[outage_attributes.time_cluster_id == cluster_id]\n",
    "    ax.bar(\n",
    "        data.days_since_data_start,\n",
    "        np.ones(len(data)) * cluster_id,\n",
    "        width=np.ones(len(data)) * 1,\n",
    "        label=cluster_id\n",
    "    )\n",
    "ax.grid(alpha=0.2)\n",
    "ax.set_xlabel(\"Time [days since start]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97217160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially cluster within each temporal cluster\n",
    "\n",
    "def geo_cluster(lat: np.ndarray, long: np.ndarray, epsilon_deg, min_samples):\n",
    "    dbscan = DBSCAN(\n",
    "        eps=np.deg2rad(epsilon_deg),\n",
    "        min_samples=min_samples,\n",
    "        metric='haversine'\n",
    "    )\n",
    "    lat_lng_pts = [x for x in zip(lat, long)]\n",
    "    dbscan.fit(np.radians(lat_lng_pts))\n",
    "    return pd.Series(dbscan.labels_)\n",
    "\n",
    "for time_cluster_id in set(outage_attributes.time_cluster_id):\n",
    "    \n",
    "    time_cluster_mask = outage_attributes.time_cluster_id == time_cluster_id\n",
    "\n",
    "    outage_attributes.loc[time_cluster_mask, \"geo_cluster_id\"] = geo_cluster(\n",
    "        outage_attributes.county_lat.values,\n",
    "        outage_attributes.county_long.values,\n",
    "        geo_epsilon_deg,  # epsilon degrees \n",
    "        geo_min_samples\n",
    "    )\n",
    "\n",
    "    outage_attributes.loc[time_cluster_mask, \"geometry\"] = gpd.points_from_xy(\n",
    "        outage_attributes.loc[time_cluster_mask, \"county_long\"],\n",
    "        outage_attributes.loc[time_cluster_mask, \"county_lat\"]\n",
    "    )\n",
    "    outage_attributes = gpd.GeoDataFrame(outage_attributes)\n",
    "    \n",
    "outage_attributes.geo_cluster_id = outage_attributes.geo_cluster_id.astype(int)\n",
    "\n",
    "# generate a unique spatio-temporal cluster id\n",
    "outage_attributes[\"cluster_id\"] = outage_attributes.apply(\n",
    "    lambda row: tuple([row.time_cluster_id, int(row.geo_cluster_id)]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "outage_attributes.cluster_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16623dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(duration_dir, exist_ok=True)\n",
    "outage_attributes.loc[:, [\"CountyFIPS\", \"start\", \"n_periods\", \"cluster_id\"]].to_csv(\n",
    "    os.path.join(\n",
    "        duration_dir,\n",
    "        f\"POUS_{resample_freq=}_{outage_threshold=}_durations.csv\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb001bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "outage_attributes.n_periods.plot(\n",
    "    kind=\"hist\",\n",
    "    bins=30,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(alpha=0.2, which=\"both\")\n",
    "ax.set_xlabel(f\"Outage duration [{resample_freq}]\")\n",
    "ax.set_title(f\"POUS distribution of outage durations, {min(years)}-{max(years)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    outage_attributes.day_of_year,\n",
    "    outage_attributes.n_periods\n",
    ")\n",
    "ax.set_xlabel(\"Day of year\")\n",
    "ax.set_ylabel(f\"Outage duration [{resample_freq}]\")\n",
    "ax.set_title(f\"POUS outage duration seasonality, {min(years)}-{max(years)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876b94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot timeseries for each event, with a little inset map of relevant counties\n",
    "\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cmap = matplotlib.colormaps['spring']\n",
    "max_plot_length = \"90D\"\n",
    "\n",
    "#for i, cluster_id in [(1, (35, 0))]:\n",
    "for i, cluster_id in enumerate(outage_attributes.cluster_id.unique()):\n",
    "    \n",
    "    if -1 in cluster_id:\n",
    "        continue  # couldn't cluster, usually noise\n",
    "    \n",
    "    event = outage_attributes[outage_attributes.cluster_id == cluster_id]\n",
    "        \n",
    "    time_cluster_id, = event.time_cluster_id.unique()\n",
    "    geo_cluster_id, = event.geo_cluster_id.unique()\n",
    "    geo_cluster_id = int(geo_cluster_id)\n",
    "        \n",
    "    f, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    ax.axhline(1 - outage_threshold, ls=\"--\", color=\"white\", label=\"Outage threshold\")\n",
    "    \n",
    "    for outage_attr in event.itertuples():\n",
    "\n",
    "        county_data_hourly: pd.DataFrame = all_counties_hourly.loc[(slice(None), outage_attr.CountyFIPS), :]\n",
    "\n",
    "        event_duration = pd.Timedelta(resample_freq) * outage_attr.n_periods\n",
    "        if event_duration > pd.Timedelta(max_plot_length):\n",
    "            continue  # do not plot counties with outages over 90 days, this is probably an error\n",
    "\n",
    "        # add a buffer around the start and end of the run            \n",
    "        event_start_datetime = pd.to_datetime(outage_attr.start)\n",
    "        plot_start: str = str((event_start_datetime - pd.Timedelta(start_buffer)).date())\n",
    "        event_end_datetime = event_start_datetime + event_duration\n",
    "        plot_end: str = str((event_end_datetime + pd.Timedelta(end_buffer)).date())\n",
    "\n",
    "        county_name, state_name, state_code = us_admin_name(outage_attr.CountyFIPS, county_boundaries, states)\n",
    "            \n",
    "        # select our hourly data to plot\n",
    "        try:\n",
    "            label_str = f\"{county_name}, {states.loc[int(state_code), 'state_alpha_2_code']}\"\n",
    "        except Exception as e:\n",
    "            label_str = f\"{county_name}, ?\"\n",
    "        timeseries = 1 - county_data_hourly.droplevel(1).loc[plot_start: plot_end, \"OutageFraction\"]\n",
    "        timeseries.plot(\n",
    "            ax=ax,\n",
    "            x_compat=True,  # enforce standard matplotlib date tick labelling \"2023-09-21\"\n",
    "            label=label_str,\n",
    "            color=cmap(hash(label_str) % 100 / 100)\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"1 - Fraction of customers in county without power\", labelpad=20)\n",
    "    ax.set_xlabel(\"Time\", labelpad=20)\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n",
    "    ax.set_ylim(-0.05, 1.1)\n",
    "    ax.grid(alpha=0.3, which=\"both\")\n",
    "    ax.set_title(f\"POUS outage cluster {cluster_id}\")\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    if len(handles) < 30:\n",
    "        ax.legend(\n",
    "            by_label.values(),\n",
    "            by_label.keys(),\n",
    "            bbox_to_anchor=(1.08, 0.98),\n",
    "            ncols=max(1, int(np.ceil(len(event) / 35))),\n",
    "            loc=\"upper right\",\n",
    "            prop={'size':7}\n",
    "        )\n",
    "     \n",
    "    plt.subplots_adjust(bottom=0.2, top=0.9, left=0.1, right=0.9)\n",
    "    \n",
    "    # inset map of county centres\n",
    "    ax_map = f.add_axes([0.73, 0.1, 0.3, 0.2]) \n",
    "    affected_counties = county_boundaries[county_boundaries.GEOID.isin(event.CountyFIPS)]\n",
    "    affected_counties.loc[:, [\"GEOID\", \"geometry\"]].merge(\n",
    "        event.loc[:, [\"CountyFIPS\", \"days_since_data_start\"]],\n",
    "        left_on=\"GEOID\",\n",
    "        right_on=\"CountyFIPS\"\n",
    "    ).plot(\n",
    "        column=\"days_since_data_start\",\n",
    "        cmap=\"Blues\",\n",
    "        ax=ax_map\n",
    "    )\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    usa = world[world.iso_a3 == \"USA\"]\n",
    "    usa.boundary.plot(ax=ax_map, alpha=0.5)\n",
    "    ax_map.grid(alpha=0.2)\n",
    "    ax_map.set_xlim(-130, -65)\n",
    "    ax_map.set_ylim(22, 53)\n",
    "    ax_map.set_ylabel(\"Latitude [deg]\")\n",
    "    ax_map.yaxis.set_label_position(\"right\")\n",
    "    ax_map.yaxis.tick_right()\n",
    "    ax_map.set_xlabel(\"Longitude [deg]\")\n",
    "    \n",
    "    # save to disk\n",
    "    f.savefig(\n",
    "        os.path.join(\n",
    "            plot_dir,\n",
    "            f\"{time_cluster_id=:d}_{geo_cluster_id=:d}_{plot_start=}_{plot_end=}.png\"\n",
    "        )\n",
    "    )\n",
    "    plt.close(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
